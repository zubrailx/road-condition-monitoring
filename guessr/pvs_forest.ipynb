{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from scipy.signal import iirfilter, lfilter\n",
    "\n",
    "tick = 25000\n",
    "window = 128\n",
    "slide = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# signal processing for input batch\n",
    "fs = 1000000 / tick\n",
    "filter_N = 4\n",
    "filter_freq = 10\n",
    "\n",
    "fft_ac_len = int(window * filter_freq / fs)\n",
    "fft_gy_len = fft_ac_len\n",
    "\n",
    "\n",
    "# noice reduction can be performed optimized on all input (but not only on window), no difference in implementation\n",
    "def reduce_noice(acDf: pandas.DataFrame, gyDf: pandas.DataFrame):\n",
    "    b, a = iirfilter(filter_N, Wn=filter_freq, fs=fs, btype=\"low\", ftype=\"butter\")\n",
    "\n",
    "    acX = lfilter(b, a, acDf.x)\n",
    "    acY = lfilter(b, a, acDf.y)\n",
    "    acZ = lfilter(b, a, acDf.z)\n",
    "\n",
    "    gyX = lfilter(b, a, gyDf.x)\n",
    "    gyY = lfilter(b, a, gyDf.y)\n",
    "    gyZ = lfilter(b, a, gyDf.z)\n",
    "\n",
    "    acDf1 = acDf.copy()\n",
    "    gyDf1 = gyDf.copy()\n",
    "    \n",
    "    acDf1[\"x\"] = acX\n",
    "    acDf1[\"y\"] = acY\n",
    "    acDf1[\"z\"] = acZ\n",
    "\n",
    "    gyDf1[\"x\"] = gyX\n",
    "    gyDf1[\"y\"] = gyY\n",
    "    gyDf1[\"z\"] = gyZ\n",
    "\n",
    "    return (acDf1, gyDf1)\n",
    "    # acil = (acXil**2 + acYil**2 + acZil**2) ** 0.5\n",
    "    # gyil = (gyXil**2 + gyYil**2 + gyZil**2) ** 0.5\n",
    "\n",
    "\n",
    "def feature_names():\n",
    "    def array_names(array_len, label_prefix):\n",
    "        return list(map(lambda x: f\"{label_prefix}_{x}\", range(0, array_len)))\n",
    "    \n",
    "    return array_names(fft_ac_len, 'acc') + array_names(fft_gy_len, 'gy') + ['acc_max', 'gy_max', 'speed']\n",
    "\n",
    "# compute magnitude on all axis, apply fast fft transformation, \n",
    "# get 32 elements of each (accelerometer, gyroscope), max acceleromet, max gyroscope\n",
    "def extract_features(acDf: pandas.DataFrame, gyDf: pandas.DataFrame, gpsDf: pandas.Series):\n",
    "    acM = (acDf[\"x\"] ** 2 + acDf[\"y\"] ** 2 + acDf[\"z\"] ** 2) ** 0.5\n",
    "    gyM = (gyDf[\"x\"] ** 2 + gyDf[\"y\"] ** 2 + gyDf[\"z\"] ** 2) ** 0.5\n",
    "    maxAc = max(acM)\n",
    "    maxGy = max(gyM)\n",
    "\n",
    "    acFft = np.fft.fft(acM)\n",
    "    gyFft = np.fft.fft(gyM)\n",
    "    # frequencies = np.fft.fftfreq(window, 1/fs)\n",
    "\n",
    "    return np.concatenate([np.abs(acFft[:fft_ac_len]), np.abs(gyFft[:fft_gy_len]), [maxAc, maxGy, gpsDf['speed']]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce noice, extract features (with fft), write\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = \"data/pvs-points/raw\"\n",
    "output_dir = \"data/pvs-forest\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "entry_fpaths = [os.path.join(raw_dir, x) for x in os.listdir(raw_dir)]\n",
    "out_file = os.path.join(output_dir, \"features-predictions.csv\")\n",
    "\n",
    "def read_entry(entry_fpath):\n",
    "  with open(entry_fpath) as file:\n",
    "      data = json.load(file)\n",
    "      acDf = pd.DataFrame.from_records(data[\"accelerometer\"])\n",
    "      gyDf = pd.DataFrame.from_records(data[\"gyroscope\"])\n",
    "      gpsDf = pd.Series(data[\"gps\"])\n",
    "      prediction = data[\"prediction\"]\n",
    "      return (acDf, gyDf, gpsDf, prediction)\n",
    "\n",
    "def display_sensors(acDf, gyDf):\n",
    "  plt.figure().set_figwidth(20)\n",
    "  plt.plot(range(0, len(acDf)), acDf[\"x\"].to_numpy(), '-', label=\"AC\")\n",
    "  plt.plot(range(0, len(gyDf)), gyDf[\"x\"].to_numpy(), '-', label=\"GY\")\n",
    "\n",
    "def display_features(features):\n",
    "  plt.figure().set_figwidth(20)\n",
    "  plt.plot(range(0, len(features)), features)\n",
    "\n",
    "features_predictions = []\n",
    "\n",
    "for entry_fpath in entry_fpaths:\n",
    "    acDf, gyDf, gpsDf, prediction = read_entry(entry_fpath)\n",
    "    acDfn, gyDfn = reduce_noice(acDf, gyDf)\n",
    "    features = extract_features(acDf, gyDf, gpsDf)\n",
    "    features_predictions.append(np.concatenate([features, [prediction]]))\n",
    "    # if prediction == 0.5:\n",
    "    #   display_sensors(acDfn, gyDfn)\n",
    "    #   display_features(features)\n",
    "    #   print(features)\n",
    "    #   break\n",
    "    \n",
    "features_prediction_df = pd.DataFrame(features_predictions, columns=(feature_names() + ['prediction']))\n",
    "features_prediction_df.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 64, 65, 32, 46, 43, 44, 42, 6, 11, 47, 45, 14, 7, 50, 40, 8, 9, 41, 10, 16, 49, 12, 13]\n",
      "['acc_0', 'acc_max', 'gy_max', 'gy_0', 'gy_14', 'gy_11', 'gy_12', 'gy_10', 'acc_6', 'acc_11', 'gy_15', 'gy_13', 'acc_14', 'acc_7', 'gy_18', 'gy_8', 'acc_8', 'acc_9', 'gy_9', 'acc_10', 'acc_16', 'gy_17', 'acc_12', 'acc_13']\n"
     ]
    }
   ],
   "source": [
    "# select only featues with least correlation\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "features_prediction_df = pd.read_csv(out_file)\n",
    "\n",
    "corr_matrix = features_prediction_df.corr()\n",
    "corr_with_prediction = corr_matrix['prediction']\n",
    "\n",
    "\n",
    "correlations = corr_with_prediction.abs().sort_values(ascending=False)\n",
    "\n",
    "# select only top k features excluding features\n",
    "k = 24\n",
    "exclude_features = [\"speed\"]\n",
    "\n",
    "postfix = \"-\".join(map(lambda x: f\"no{x}\", exclude_features))\n",
    "if len(postfix) > 0:\n",
    "  postfix = \"-\" + postfix\n",
    "            \n",
    "selected_feature_indexes = correlations.drop(['prediction', *exclude_features])[:k].index\n",
    "\n",
    "out_features_predictions_selected_fpath = os.path.join(output_dir, f\"selected-features-predictions-{k}{postfix}.csv\")\n",
    "features_prediction_df_selected = features_prediction_df[[*selected_feature_indexes, 'prediction']]\n",
    "features_prediction_df_selected.to_csv(out_features_predictions_selected_fpath, index=False)\n",
    "\n",
    "# save indexes and names of selected features\n",
    "with open(os.path.join(\"model\", f\"selected-features-{k}{postfix}.json\"), \"w\") as file:\n",
    "  lst = list(map(lambda x: list(features_prediction_df.columns).index(x), selected_feature_indexes))\n",
    "  json.dump(lst, file)\n",
    "  print(lst)\n",
    "\n",
    "with open(os.path.join(\"model\", f\"selected-features-names-{k}{postfix}.json\"), \"w\") as file:\n",
    "  lst = list(selected_feature_indexes)\n",
    "  json.dump(lst, file)\n",
    "  print(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(out_features_predictions_selected_fpath)\n",
    "clf = RandomForestRegressor(criterion='squared_error', n_estimators=100, max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "data_shuffled = features_prediction_df_selected.sample(frac=1, random_state=5)\n",
    "\n",
    "X = data_shuffled.drop(['prediction'], axis=1)\n",
    "y = data_shuffled['prediction']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = clf.fit(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_dir = \"model\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "with open(os.path.join(model_dir, f\"forest-features-{k}{postfix}.pickle\"), \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def regression_results(y_pred, y_true):\n",
    "    # Regression metrics\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "\n",
    "    print('R2: ', round(r2,4)) \n",
    "    print('Explained variance (biased): ', round(explained_variance,4)) # if mean(error)=0, then R2 = explained variance score\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    print('MSLE: ', round(mean_squared_log_error,4)) # mean squared logarithmic error\n",
    "\n",
    "def visualize_sorted_results(y_pred: numpy.array, y_true: numpy.array):\n",
    "    y_true_ids = y_true.argsort()\n",
    "    y_true_sorted = y_true[y_true_ids[::-1]]\n",
    "    y_pred_sorted = y_pred[y_true_ids[::-1]]\n",
    "    l = len(y_true_sorted)\n",
    "\n",
    "    plt.figure().set_figwidth(15)\n",
    "    plt.scatter(range(0, l), y_true_sorted, label=\"True\", s=5)\n",
    "    plt.scatter(range(0, l), y_pred_sorted, label=\"Predicted\", s=1)\n",
    "\n",
    "def distribution_results(y_pred: numpy.array, y_true: numpy.array):\n",
    "    total = 10\n",
    "    step = 1 / total\n",
    "    for i in range(0, total + 1):\n",
    "        lower_bound = i * step\n",
    "        upper_bound = (i + 1) * step\n",
    "        y_true_mask = (y_true >= lower_bound) & (y_true < upper_bound)\n",
    "        y_pred_dist = [0] * total\n",
    "        for j in range(0, len(y_true)):\n",
    "            if y_true_mask[j] == False:\n",
    "                continue\n",
    "            bucket = int((y_pred[j] - 0.001) / step)\n",
    "            y_pred_dist[bucket] += 1\n",
    "\n",
    "        cnt = y_true_mask.sum()\n",
    "        if cnt != 0:\n",
    "            rounded_distribution = [round(v/cnt, 3) for v in y_pred_dist]\n",
    "            print(f\"[{round(lower_bound,2)}...{round(upper_bound, 2)}]={cnt}:\\t{'/'.join(map(str, rounded_distribution))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
